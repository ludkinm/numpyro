{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 schools data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import Predictive\n",
    "from numpyro.infer.reparam import TransformReparam, LocScaleReparam\n",
    "from jax import random\n",
    "from numpyro.infer import MCMC\n",
    "from numpyro.infer.hug import Hug\n",
    "import numpyro.distributions as dist\n",
    "import numpyro\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore NumPyro using a simple example. We will use the eight schools example from Gelman et al., Bayesian Data Analysis: Sec. 5.5, 2003, which studies the effect of coaching on SAT performance in eight schools.\n",
    "\n",
    "The data is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 8\n",
    "y = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\n",
    "sigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `y` are the treatment effects and `sigma` the standard error. \n",
    "\n",
    "We build a hierarchical model for the study where we assume that the group-level parameters `theta` for each school are sampled from a Normal distribution with unknown mean `mu` and standard deviation `tau`, while the observed data are in turn generated from a Normal distribution with mean and standard deviation given by `theta` (true effect) and `sigma`, respectively. \n",
    "This allows us to estimate the population-level parameters `mu` and `tau` by pooling from all the observations, while still allowing for individual variation amongst the schools using the group-level `theta` parameters.\n",
    "This is written in `numpyro` using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eight_schools(J, sigma, y=None):\n",
    "    mu = numpyro.sample('mu', dist.Normal(0, 5))\n",
    "    tau = numpyro.sample('tau', dist.HalfCauchy(5))\n",
    "    with numpyro.plate('J', J):\n",
    "        theta = numpyro.sample('theta', dist.Normal(mu, tau))\n",
    "        numpyro.sample('obs', dist.Normal(theta, sigma), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us infer the values of the unknown parameters in our model by running MCMC using the No-U-Turn Sampler (NUTS). Note the usage of the extra_fields argument in MCMC.run. By default, we only collect samples from the target (posterior) distribution when we run inference using MCMC. However, collecting additional fields like potential energy or the acceptance probability of a sample can be easily achieved by using the extra_fields argument. For a list of possible fields that can be collected, see the `HMCState` object. In this example, we will additionally collect the `potential_energy` for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "kernel = Hug(eight_schools, step_size=0.1, trajectory_length=10)\n",
    "mcmc = MCMC(kernel, num_warmup=0, num_samples=1000, num_chains=1)\n",
    "rng_key = random.PRNGKey(0)\n",
    "mcmc.run(rng_key, J, sigma, y=y, extra_fields=('potential_energy','accept_prob'))\n",
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = mcmc.get_extra_fields()['potential_energy']\n",
    "print('Expected log joint density: {:.2f}'.format(\n",
    "    np.mean(-pe)))  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = mcmc.get_extra_fields()['accept_prob']\n",
    "plt.hist(ap, range=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mcmc.get_samples()\n",
    "plt.plot(samples['theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples['tau'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples['mu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values above 1 for the split Gelman Rubin diagnostic `r_hat` indicates that the chain has not fully converged. The low value for the effective sample size `n_eff`, particularly for `tau`, and the number of divergent transitions looks problematic. \n",
    "Fortunately, this is a common pathology that can be rectified by using a non-centered paramaterization for `tau` in our model. This is straightforward to do in `numpyro` by using a `TransformedDistribution` instance together with a \"reparameterization effect handler\". Let us rewrite the same model but instead of sampling `theta` from a Normal(`mu`, `tau`), we will instead sample it from a base Normal(0, 1) distribution that is transformed using an `AffineTransform`. \n",
    "Note that by doing so, `nunmpyro` runs HMC by generating samples `theta_base` for the base Normal(0, 1) distribution instead. We see that the resulting chain does not suffer from the same pathology â€” the Gelman Rubin diagnostic is 1 for all the parameters and the effective sample size looks quite good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eight_schools_noncentered(J, sigma, y=None):\n",
    "    mu = numpyro.sample('mu', dist.Normal(0, 5))\n",
    "    tau = numpyro.sample('tau', dist.HalfCauchy(5))\n",
    "    with numpyro.plate('J', J):\n",
    "        with numpyro.handlers.reparam(config={'theta': TransformReparam()}):\n",
    "            theta = numpyro.sample(\n",
    "                'theta',\n",
    "                dist.TransformedDistribution(dist.Normal(0., 1.),\n",
    "                                             dist.transforms.AffineTransform(mu, tau)))\n",
    "        numpyro.sample('obs', dist.Normal(theta, sigma), obs=y)\n",
    "\n",
    "kernel = Hug(eight_schools_noncentered, step_size=0.1)\n",
    "mcmc = MCMC(kernel, num_warmup=500, num_samples=1000)\n",
    "rng_key = random.PRNGKey(0)\n",
    "mcmc.run(rng_key, J, sigma, y=y, extra_fields=('potential_energy',))\n",
    "mcmc.print_summary(exclude_deterministic=False)\n",
    "pe = mcmc.get_extra_fields()['potential_energy']\n",
    "print('Expected log joint density: {:.2f}'.format(\n",
    "    np.mean(-pe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mcmc.get_samples()\n",
    "plt.plot(samples['theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples['tau'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples['mu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assume that we have a new school for which we have not observed any test scores, but we would like to generate predictions. `numpyro` provides a `Predictive` class for such a purpose. Note that in the absence of any observed data, we simply use the population-level parameters to generate predictions. The `Predictive` utility conditions the unobserved `mu` and `tau` sites to values drawn from the posterior distribution from our last MCMC run, and runs the model forward to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_school():\n",
    "    mu = numpyro.sample('mu', dist.Normal(0, 5))\n",
    "    tau = numpyro.sample('tau', dist.HalfCauchy(5))\n",
    "    return numpyro.sample('obs', dist.Normal(mu, tau))\n",
    "\n",
    "predictive = Predictive(new_school, mcmc.get_samples())\n",
    "samples_predictive = predictive(random.PRNGKey(1))\n",
    "print(np.mean(samples_predictive['obs']))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
